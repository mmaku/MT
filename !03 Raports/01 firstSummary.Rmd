---
title: "Short gLASSO & gSLOPE summary"
author: "Micha³ Makowski"
date: "13 July 2018"
output: 
  html_document: 
    df_print: tibble
    highlight: tango
    toc: yes
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE)

setwd("..")
source("12 simulationsFunctions.R")
setwd("./!03 Raports/")

require(dplyr, quietly = TRUE)
require(knitr, quietly = TRUE)
require(kableExtra, quietly = TRUE)

load("../!02 Data/01 Binded/Simulation_10_18_07_18_12_27.RData")
output %>%
    # filter((procedure == "gLASSO" & alpha > 0.04) | (procedure == "BHgSLOPE" & alpha < 0.04)) %>%
    select(-penalizeDiagonal, -partial, -iterations, -SP) %>%
    rename(Power = SN) %>%
    mutate_if(is.numeric, funs(round), 4) %>%
    arrange(n) -> sanityCheck

load("../!02 Data/01 Binded/Simulation_30_18_07_18_14_27.RData")
output %>%
    filter(graphType == "hub" & alpha > 0.05 & n %in% c(100, 200, 300)) %>%
    select(-penalizeDiagonal, -partial, -iterations, -SP) %>%
    rename(Power = SN) %>%
    arrange(n) %>%
    mutate_if(is.numeric, funs(round), 4) -> hub0.05

output %>%
    filter(graphType == "hub" & alpha == 0.1 & n %in% c(100, 200, 300)) %>%
    select(-penalizeDiagonal, -partial, -iterations, -SP) %>%
    rename(Power = SN) %>%
    arrange(n) %>%
    mutate_if(is.numeric, funs(round), 4) -> hub0.1

output %>%
    filter(graphType == "cluster" & alpha == 0.05 & n %in% c(100, 200, 300)) %>%
    select(-penalizeDiagonal, -partial, -iterations, -SP) %>%
    rename(Power = SN) %>%
    arrange(n) %>%
    mutate_if(is.numeric, funs(round), 4) -> cluster0.05

output %>%
    filter(graphType == "cluster" & alpha == 0.1 & n %in% c(100, 200, 300)) %>%
    select(-penalizeDiagonal, -partial, -iterations, -SP) %>%
    rename(Power = SN) %>%
    arrange(n) %>%
    mutate_if(is.numeric, funs(round), 4) -> cluster0.1

output %>%
    rename(Power = SN, Specificity = SP) %>%
    arrange(graphType) %>%
    select(-penalizeDiagonal, -partial, -iterations, -n, -alpha, -graphType, -p) %>%
    mutate_if(is.numeric, funs(round), 4) -> full


```

## Introduction

This short document describe a little of my work around graphical models. We will not dive deeply into graphical models assuming that the reader is quite familiar with the concept of graphical models, convex optimization and ADMM algorithm. We will 
present current finding and problems.

In all next paragraphs the following notation is used:

* $n$ is sample size,
* $p$ is dimension of graph (i.e. maximal number of nodes, number of observed random variables),
* $\lambda$ is regularizer (could be sequence),
* $F_n(\alpha)$ is quantile function of t-student distribution,
* $\sigma_i$ is the variance of $i$-th variable,

## Implementation

We currently implemented ADMM for gLASSO and gSLOPE. Although gLASSO with ADMM is working correctly, during preparation of this document We used gLASSO implementation from the __glasso__ package due to faster computation times. 
For a data simulation We use the __huge__ R package that contains the function for generating data from gaussian RMF, as well as a function for the precision matrix estimation. 
It contains some interesting methods of finding a proper graph structure based on _StARS_, _eBIC_ and _RIC_ ([link to  vignette](https://cran.r-project.org/web/packages/huge/vignettes/vignette.pdf)). 
We would like to compare those methods with our results, but that is not prepared yet.

All my code is hosted on GitHub: https://github.com/mmaku/MT.

## Lambda sequence

We use 4 diffrent setups of regularizations parameters, 1 for gLASSO and 2 for gSLOPE.

### gLASSO Banerjee

First regularizer for gLASSO is based on Banerjee et al. work from 2008. The formula for $\lambda$ is given by

\[\lambda^{Banerjee} := \max_{i<j}(\sigma_i, \sigma_j) \frac{F_{n-2}\large(1-\frac{\alpha}{2p^2}\large)}{\sqrt{n-2+F^2_{n-2}\large(1-\frac{\alpha}{2p^2}\large)}}.\]

Notation as at the begining of the document.

### gSLOPE BH

gSLOPE regularizer based on Benjamini-Hochberg and suggested by Ma³gorzata Bogdan and Piotr Sobczyk is given by
\[\begin{align*}
m & := {p \choose 2} = \frac{p(p-1)}{2},\\
\lambda^{BH}_k & :=  \frac{F_{n-2}\large(1-\frac{\alpha}{m+1-k}\large)}{\sqrt{n-2+F^2_{n-2}\large(1-\frac{\alpha}{m+1-k)}\large)}},\\
\lambda^{BH} & := \{\underbrace{\lambda^{BH}_1,...,\lambda^{BH}_1}_\text{p times for diagonal},\lambda^{BH}_1,\lambda^{BH}_1,\lambda^{BH}_2,\lambda^{BH}_2,...,\lambda^{BH}_m,\lambda^{BH}_m\}
\end{align*}\]

Piotr suggested adjusting BH sequence that the first term is equal to $\lambda^{Bonferonni}$ and use it as a sanity check for gSLOPE, it should have higher power than gLASSO with $\lambda^{Bonferonni}$.

### gSLOPE Holm

Also suggested by them, regularizer based on Holm-Bonferonni procedure
\[\begin{align*}
m & := {p \choose 2} = \frac{p(p-1)}{2},\\
\lambda^{Holm}_k & :=  \frac{F_{n-2}\large(1-\frac{k*\alpha}{m}\large)}{\sqrt{n-2+F^2_{n-2}\large(1-\frac{k*\alpha}{m)}\large)}},\\
\lambda^{Holm} & := \{\underbrace{\lambda^{Holm}_1,...,\lambda^{Holm}_1}_\text{p times for diagonal},\lambda^{Holm}_1,\lambda^{Holm}_1,\lambda^{Holm}_2,\lambda^{Holm}_2,...,\lambda^{Holm}_m,\lambda^{Holm}_m\}
\end{align*}\]

### Centering

In both cases data must be scaled and centered before calculating the covariance matrix. In calculation of $\lambda^{Banerjee}$ there is term $\max_{i<j}(\sigma_i, \sigma_j)$ responsible for adjusting the magnitude of the parameter. 
We did consider about such term for gSLOPE, maybe we should discuss the idea of it with Piotr. We did not think about derivations and difficulties a lot.

### Symmetry

Sequences showed are used in procedure, when whole matrix is treated as a set of variables. Usually we use only upper triangle of the matrix, and then, the sequence has following form 

\[\lambda^{A} := \{\underbrace{\lambda^{A}_1,..........,\lambda^{A}_1}_{\substack{\text{p times, only when} \\ \text{diagonal is penalized}}},\lambda^{A}_1,\lambda^{A}_2,...,\lambda^{A}_m\}\]


## Simulations 

The __huge__ package provides very convinient way of data generation - we could define the graph structure, number of observations $n$, number of nodes $p$, the relation beetwen off and on diagonal entries of the precision matrix.

Currently we are using quite standard parameters - the proper estimations of many types of graphs takes a lot of computional time. 
Also, due to long computatations we used only 250 iterations to compute FDR and power (1 iteration = 1 precision matrix estimation = 3 estimations with diffrent methods).

We conducted some simulations earlier - same setup, different number of iterations (10, 20, 40, 80, 160, 620, 1200, 2500 and 5000) FDR and power seems to stabilize after 100 iterations, but in some setups there is diffrence beetween 2500 and 5000 iterations after 3rd decimal place. Keeping this in mind results may not be best estimators of FDR and power, but they are consistent and we could compare them.

Below we attach tables with results, we do not comment them a lot. For the FDR and power calcutation we used only upper triangle of precision matrix (without diagonal) - as diagonal is obvious choice of precision matrix, it may give us better results in term of FDR and power.

### Sanity check

We adjust first $\lambda$ in BH sequence for gSLOPE to be equal to $\lambda$ for gLASSO. Power for gSLOPE is higher, but but FDR is also much higher, both measures are almost two times larger than for gLASSO.

```{r sanityCheck}

sanityCheck %>%
    kable(align='lcccccc') %>%
    row_spec(c(2,4,6,8), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

```


Now we will present results for some specific setups, it seems that gSLOPE with BH sequence does not hold FDR bound.

### Graph = "cluster", $\alpha$ = 0.1, n $\in$ {100, 200, 300}, p = 200 

```{r cluster0.1}

cluster0.1 %>%
    kable(align='lcccccc') %>%
    row_spec(c(3,6), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

```

### Graph = "cluster", $\alpha$ = 0.05, n $\in$ {100, 200, 300}, p = 200 

```{r cluster0.05}

cluster0.05 %>%
    kable(align='lcccccc') %>%
    row_spec(c(3,6), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

```

### Graph = "hub", $\alpha$ = 0.1, n $\in$ {100, 200, 300}, p = 200 

```{r hub0.1}

hub0.1 %>%
    kable(align='lcccccc') %>%
    row_spec(c(3,6), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

```

### Graph = "hub", $\alpha$ = 0.05, n $\in$ {100, 200, 300}, p = 200 

```{r hub0.05}

hub0.05 %>%
    kable(align='lcccccc') %>%
    row_spec(c(3,6), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

```


### Plots

Below I attached 3 plots to illustrate how gSLOPE and gLASSO identify nodes. 
I changed setup for a better visibility,

```{r plots}

n = 100
p = 100
graphType = "cluster"
alpha = 0.05
penalizeDiagonal = FALSE

banerjeeLambda <- lambdaSelector(input = p, n = n, alpha = alpha, method = "banerjee", verbose = FALSE)
holmlambda <- lambdaSelector(input = p, n = n, alpha = alpha, method = "holm", verbose = FALSE)
BHlambda <- lambdaSelector(input = p, n = n, alpha = alpha, method = "BH", verbose = FALSE) 

generatedData <- huge.generator(n, d = p, graph = graphType, verbose = FALSE) 

omegaHat <- glasso(s = generatedData$sigmahat, rho = banerjeeLambda,
                    penalize.diagonal = penalizeDiagonal)$wi

plotDiffrence(omegaHat, generatedData$theta, method = "gLASSO", graphType = graphType, p = p, n = n, alpha = alpha)

omegaHat <- gslopeADMM(sampleCovariance = generatedData$sigmahat, lambda = holmlambda,
                        penalizeDiagonal = penalizeDiagonal,
                        truncate = TRUE, verbose = FALSE)$precisionMatrix

plotDiffrence(omegaHat, generatedData$theta, method = "Holm gSLOPE", graphType = graphType, p = p, n = n, alpha = alpha)

omegaHat <- gslopeADMM(sampleCovariance = generatedData$sigmahat, lambda = BHlambda,
                        penalizeDiagonal = penalizeDiagonal,
                        truncate = TRUE, verbose = FALSE)$precisionMatrix

plotDiffrence(omegaHat, generatedData$theta, method = "BH gSLOPE", graphType = graphType, p = p, n = n, alpha = alpha)

```

### All results

In all settings below p = 200.

```{r full}

full %>%
    kable(align='lccc') %>%
    group_rows("Type = \"cluster\", n = 100, alpha = 0.1", 1, 3) %>%
    group_rows("Type = \"cluster\", n = 150, alpha = 0.1", 4, 6) %>%
    group_rows("Type = \"cluster\", n = 200, alpha = 0.1", 7, 9) %>%
    group_rows("Type = \"cluster\", n = 250, alpha = 0.1", 10, 12) %>%
    group_rows("Type = \"cluster\", n = 300, alpha = 0.1", 13, 15) %>%
    group_rows("Type = \"cluster\", n = 100, alpha = 0.05", 16, 18) %>%
    group_rows("Type = \"cluster\", n = 150, alpha = 0.05", 19, 21) %>%
    group_rows("Type = \"cluster\", n = 200, alpha = 0.05", 22, 24) %>%
    group_rows("Type = \"cluster\", n = 250, alpha = 0.05", 25, 27) %>%
    group_rows("Type = \"cluster\", n = 300, alpha = 0.05", 28, 30) %>%
    group_rows("Type = \"cluster\", n = 100, alpha = 0.01", 31, 33) %>%
    group_rows("Type = \"cluster\", n = 150, alpha = 0.01", 34, 36) %>%
    group_rows("Type = \"cluster\", n = 200, alpha = 0.01", 37, 39) %>%
    group_rows("Type = \"cluster\", n = 250, alpha = 0.01", 40, 42) %>%
    group_rows("Type = \"cluster\", n = 300, alpha = 0.01", 43, 45) %>%
    group_rows("Type = \"hub\", n = 100, alpha = 0.1", 46, 48) %>%
    group_rows("Type = \"hub\", n = 150, alpha = 0.1", 49, 51) %>%
    group_rows("Type = \"hub\", n = 200, alpha = 0.1", 52, 54) %>%
    group_rows("Type = \"hub\", n = 250, alpha = 0.1", 55, 57) %>%
    group_rows("Type = \"hub\", n = 300, alpha = 0.1", 58, 60) %>%
    group_rows("Type = \"hub\", n = 100, alpha = 0.05", 61, 63) %>%
    group_rows("Type = \"hub\", n = 150, alpha = 0.05", 64, 66) %>%
    group_rows("Type = \"hub\", n = 200, alpha = 0.05", 67, 69) %>%
    group_rows("Type = \"hub\", n = 250, alpha = 0.05", 70, 72) %>%
    group_rows("Type = \"hub\", n = 300, alpha = 0.05", 73, 75) %>%
    group_rows("Type = \"hub\", n = 100, alpha = 0.01", 76, 78) %>%
    group_rows("Type = \"hub\", n = 150, alpha = 0.01", 79, 81) %>%
    group_rows("Type = \"hub\", n = 200, alpha = 0.01", 82, 84) %>%
    group_rows("Type = \"hub\", n = 250, alpha = 0.01", 85, 87) %>%
    group_rows("Type = \"hub\", n = 300, alpha = 0.01", 88, 90) %>%
    row_spec(c(3,6), hline_after = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```


